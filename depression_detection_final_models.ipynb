{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCi4_bN25gr1",
        "outputId": "bba5c06e-09c7-4dd4-93f8-3aa33220b0a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from textblob import TextBlob\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKiHpf5357ZH",
        "outputId": "4faacf0c-4b88-4284-ac75-42cc4c156acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    text  class\n",
            "0      It's just over 2 years since I was diagnosed w...      1\n",
            "1      It's Sunday, I need a break, so I'm planning t...      1\n",
            "2      Awake but tired. I need to sleep but my brain ...      1\n",
            "3      RT @SewHQ: #Retro bears make perfect gifts and...      1\n",
            "4      Itâs hard to say whether packing lists are m...      1\n",
            "...                                                  ...    ...\n",
            "19995              A day without sunshine is like night.      0\n",
            "19996  Boren's Laws: (1) When in charge, ponder. (2) ...      0\n",
            "19997  The flow chart is a most thoroughly oversold p...      0\n",
            "19998  Ships are safe in harbor, but they were never ...      0\n",
            "19999     Black holes are where God is dividing by zero.      0\n",
            "\n",
            "[20000 rows x 2 columns]\n",
            "                                                    text        class\n",
            "0      It's just over 2 years since I was diagnosed w...      suicide\n",
            "1      It's Sunday, I need a break, so I'm planning t...      suicide\n",
            "2      Awake but tired. I need to sleep but my brain ...      suicide\n",
            "3      RT @SewHQ: #Retro bears make perfect gifts and...      suicide\n",
            "4      Itâs hard to say whether packing lists are m...      suicide\n",
            "...                                                  ...          ...\n",
            "19995              A day without sunshine is like night.  non-suicide\n",
            "19996  Boren's Laws: (1) When in charge, ponder. (2) ...  non-suicide\n",
            "19997  The flow chart is a most thoroughly oversold p...  non-suicide\n",
            "19998  Ships are safe in harbor, but they were never ...  non-suicide\n",
            "19999     Black holes are where God is dividing by zero.  non-suicide\n",
            "\n",
            "[20000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "path = \"/content/drive/MyDrive/Mental-Health-Twitter.csv\"\n",
        "\n",
        "columns_to_read = ['post_text', 'label']\n",
        "\n",
        "df = pd.read_csv(path, encoding='unicode_escape', usecols=columns_to_read)\n",
        "df.rename(columns={'label': 'class'}, inplace=True)\n",
        "df.rename(columns={'post_text': 'text'}, inplace=True)\n",
        "# num_rows_to_keep = 1000\n",
        "\n",
        "# # # Create a new DataFrame with only the specified number of rows\n",
        "# df = data.iloc[:num_rows_to_keep]\n",
        "print(df)\n",
        "\n",
        "mapping = {1: \"suicide\", 0: \"non-suicide\"}\n",
        "\n",
        "df['class'] = df['class'].map(mapping)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huoZ3ci_v0uu"
      },
      "outputs": [],
      "source": [
        "# trying to add one more feature\n",
        "\n",
        "\n",
        "\n",
        "# def contains_self_centric_words(text):\n",
        "#     self_centric_words = [\n",
        "#     \"i\", \"me\", \"myself\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\",\n",
        "#     \"self\", \"myself\"]\n",
        "\n",
        "#     tokens = text.split()\n",
        "#     for word in tokens:\n",
        "#         if word.lower() in self_centric_words:\n",
        "#             return True\n",
        "#     return False\n",
        "\n",
        "# data['label_show'] = data['text'].apply(contains_self_centric_words)\n",
        "\n",
        "# data.to_csv('updated_dataset.csv', index=False)\n",
        "\n",
        "# df = pd.read_csv('/content/updated_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO0HEB-p8BEk"
      },
      "outputs": [],
      "source": [
        "df['class'].dropna(inplace=True)\n",
        "df.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['text'] = df['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4wb7OJTwrpc",
        "outputId": "2296a291-dd5b-44de-b7e1-e97f692a6240"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    text        class\n",
            "0      2 year since diagnosed anxiety depression toda...      suicide\n",
            "1      sunday need break im planning spend little tim...      suicide\n",
            "2                      awake tired need sleep brain idea      suicide\n",
            "3      rt sewhq retro bear make perfect gift great be...      suicide\n",
            "4      itâs hard say whether packing list making life...      suicide\n",
            "...                                                  ...          ...\n",
            "19995                    day without sunshine like night  non-suicide\n",
            "19996  borens law 1 charge ponder 2 trouble delegate ...  non-suicide\n",
            "19997  flow chart thoroughly oversold piece program d...  non-suicide\n",
            "19998                  ship safe harbor never meant stay  non-suicide\n",
            "19999                       black hole god dividing zero  non-suicide\n",
            "\n",
            "[20000 rows x 2 columns]\n",
            "True Count: 10000\n",
            "False Count: 10000\n"
          ]
        }
      ],
      "source": [
        "print(df)\n",
        "\n",
        "# occurrences_counts = df['label_show'].value_counts()\n",
        "\n",
        "# # Print the counts\n",
        "# print(\"True Count:\", occurrences_counts[True])\n",
        "# print(\"False Count:\", occurrences_counts[False])\n",
        "\n",
        "occurrences_counts = df['class'].value_counts()\n",
        "\n",
        "# Print the counts\n",
        "print(\"True Count:\", occurrences_counts[\"suicide\"])\n",
        "print(\"False Count:\", occurrences_counts[\"non-suicide\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2k59MjP7QhR",
        "outputId": "30fd256c-1d8f-4b34-e2f9-49168bb9e0b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                    text        class\n",
            "0      2 year since diagnosed anxiety depression toda...      suicide\n",
            "1      sunday need break im planning spend little tim...      suicide\n",
            "2                      awake tired need sleep brain idea      suicide\n",
            "3      rt sewhq retro bear make perfect gift great be...      suicide\n",
            "4      itâs hard say whether packing list making life...      suicide\n",
            "...                                                  ...          ...\n",
            "19995                    day without sunshine like night  non-suicide\n",
            "19996  borens law 1 charge ponder 2 trouble delegate ...  non-suicide\n",
            "19997  flow chart thoroughly oversold piece program d...  non-suicide\n",
            "19998                  ship safe harbor never meant stay  non-suicide\n",
            "19999                       black hole god dividing zero  non-suicide\n",
            "\n",
            "[20000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)\n",
        "\n",
        "label_mapping = {'suicide': 1, 'non-suicide': 0}\n",
        "# show_mapping = {True:1, False:0}\n",
        "df['class'] = df['class'].replace(label_mapping)\n",
        "# df['label_show'] = df['label_show'].replace(show_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGHp3yQTyHmn"
      },
      "outputs": [],
      "source": [
        "# have to combine those 2 columns in single\n",
        "\n",
        "\n",
        "\n",
        "# X = data['text']\n",
        "# y = data[['class', 'label_show']]  # Combine both label columns into a single dataframe\n",
        "\n",
        "# # Split the dataset into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Initialize TF-IDF vectorizer\n",
        "# tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# # Fit and transform the training data\n",
        "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# # Transform the testing data\n",
        "# X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# # Initialize and train the Multinomial Naive Bayes classifier\n",
        "# mnb = MultinomialNB()\n",
        "# mnb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# # Make predictions\n",
        "# predictions = mnb.predict(X_test_tfidf)\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = accuracy_score(y_test, predictions)\n",
        "# print('Accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLqQgCME6U6p",
        "outputId": "b93d5993-0319-4424-93c2-5b364ede9e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Set Accuracy: 0.871\n",
            "Validation Set Accuracy: 0.864\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# First, ensure the dataset has sufficient data for splitting\n",
        "assert len(df) > 2, \"Dataset is too small for splitting.\"\n",
        "\n",
        "# Initial split: 60% training, 40% for validation/testing\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df['text'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the size of X_temp to confirm it has enough data for further split\n",
        "assert len(X_temp) >= 4, \"Insufficient data for 20-20 split.\"\n",
        "\n",
        "# Split the 40% into 20% validation and 20% testing\n",
        "X_validation, X_test, y_validation, y_test = train_test_split(X_temp,y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create a TF-IDF vectorizer and transform the training, validation, and test sets\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_validation_tfidf = tfidf_vectorizer.transform(X_validation)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train the Multinomial Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate accuracy for the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n",
        "\n",
        "# Validate the model's performance on the validation set\n",
        "y_val_pred = classifier.predict(X_validation_tfidf)\n",
        "val_accuracy = accuracy_score(y_validation, y_val_pred)\n",
        "print(\"Validation Set Accuracy:\", val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEIBWAqbdfHI",
        "outputId": "01d789b6-34ae-4e47-af0c-7cbd9fdb0691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation Accuracy: 0.8626874999999998\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform k-fold cross-validation with 5 folds\n",
        "cross_val_accuracy = cross_val_score(classifier, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Output cross-validation results\n",
        "print(\"Cross-Validation Accuracy:\", cross_val_accuracy.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-9QDY7xBBep",
        "outputId": "00136f87-e72f-4473-d606-be808ea34a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.8727841351878595\n",
            "Recall: 0.871\n",
            "F1-score: 0.8706202947274391\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jCcE3E1uYNc",
        "outputId": "be2a4cb0-b7b8-4744-e474-fd12a2eee6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "pip install textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xexUfoJZffxG",
        "outputId": "19c1dc0e-bc43-44b1-c7e2-a75f962b5af9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best alpha value: 0.1\n",
            "Test Set Accuracy: 0.881\n",
            "Precision: 0.871866295264624\n",
            "Recall: 0.9037536092396535\n",
            "F1 Score: 0.8875236294896031\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from textblob import TextBlob\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Function to get sentiment polarity\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "# Separate features and labels\n",
        "X = df['text']\n",
        "y = df['class']\n",
        "\n",
        "# Initial split: 60% training, 40% test/validation\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the test/validation data into 20% validation and 20% testing\n",
        "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_validation_tfidf = tfidf_vectorizer.transform(X_validation)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Get sentiment scores for text data\n",
        "X_train_sentiment = np.array([get_sentiment(text) for text in X_train]).reshape(-1, 1)\n",
        "X_validation_sentiment = np.array([get_sentiment(text) for text in X_validation]).reshape(-1, 1)\n",
        "X_test_sentiment = np.array([get_sentiment(text) for text in X_test]).reshape(-1, 1)\n",
        "\n",
        "# Scale sentiment scores using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_sentiment_scaled = scaler.fit_transform(X_train_sentiment)\n",
        "X_validation_sentiment_scaled = scaler.transform(X_validation_sentiment)\n",
        "X_test_sentiment_scaled = scaler.transform(X_test_sentiment)\n",
        "\n",
        "# Combine TF-IDF features with scaled sentiment scores\n",
        "X_train_combined = hstack((X_train_tfidf, sp.csr_matrix(X_train_sentiment_scaled)))\n",
        "X_validation_combined = hstack((X_validation_tfidf, sp.csr_matrix(X_validation_sentiment_scaled)))\n",
        "X_test_combined = hstack((X_test_tfidf, sp.csr_matrix(X_test_sentiment_scaled)))\n",
        "\n",
        "# Parameter grid for hyperparameter tuning\n",
        "param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0]}\n",
        "\n",
        "# Perform GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_combined, y_train)\n",
        "\n",
        "# Best alpha value from GridSearchCV\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "# Train the MNB model with the best alpha\n",
        "MNB_model = MultinomialNB(alpha=best_alpha)\n",
        "MNB_model.fit(X_train_combined, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = MNB_model.predict(X_test_combined)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best alpha value:\", best_alpha)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cY_6xrRfwhg",
        "outputId": "a89b27c2-071d-4efe-8588-bc5b31dd550a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Set Accuracy: 0.87\n"
          ]
        }
      ],
      "source": [
        "y_val_pred = MNB_model.predict(X_validation_combined)\n",
        "\n",
        "# Calculate validation set accuracy\n",
        "validation_accuracy = accuracy_score(y_validation, y_val_pred)\n",
        "\n",
        "print(\"Validation Set Accuracy:\", validation_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-D20_1wnDG4",
        "outputId": "627e1010-dc66-41ac-be10-dd65177cea79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation Accuracy: 0.8729375000000001\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform k-fold cross-validation with 5 folds\n",
        "cross_val_accuracy = cross_val_score(MNB_model, X_train_combined, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Output cross-validation results\n",
        "print(\"Cross-Validation Accuracy:\", cross_val_accuracy.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_-EneFlJhKi",
        "outputId": "d33eb978-59eb-4ed3-aedf-7b5cd3d333e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Depression: 0\n",
            "Depression: 1\n",
            "Depression: 0\n",
            "Depression: 1\n",
            "Depression: 0\n",
            "Depression: 0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# First, ensure the dataset has sufficient data for splitting\n",
        "assert len(df) > 2, \"Dataset is too small for splitting.\"\n",
        "\n",
        "# Initial split: 60% training, 40% for validation/testing\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df['text'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the size of X_temp to confirm it has enough data for further split\n",
        "assert len(X_temp) >= 4, \"Insufficient data for 20-20 split.\"\n",
        "\n",
        "# Split the 40% into 20% validation and 20% testing\n",
        "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create a TF-IDF vectorizer and transform the training, validation, and test sets\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_validation_tfidf = tfidf_vectorizer.transform(X_validation)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train the Multinomial Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Define a function to preprocess user input\n",
        "def preprocess_text(text):\n",
        "    # Implement your text preprocessing steps here\n",
        "    return text\n",
        "\n",
        "# Define a function to predict depression based on user input\n",
        "def predict_depression(user_input):\n",
        "    preprocessed_input = preprocess_text(user_input)\n",
        "    input_tfidf = tfidf_vectorizer.transform([preprocessed_input])\n",
        "    prediction = classifier.predict(input_tfidf)[0]\n",
        "    return prediction\n",
        "\n",
        "# Define the while loop for continuous user input\n",
        "while True:\n",
        "    user_input = input(\"Enter a sentence to check for depression (or type 'exit' to quit): \")\n",
        "\n",
        "    # Check if the user wants to exit\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Make prediction based on user input\n",
        "    prediction = predict_depression(user_input)\n",
        "\n",
        "    # Print the prediction result\n",
        "    print(\"Depression:\", prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "sjoUGnJt-o0N",
        "outputId": "53740d5d-3800-4aab-ee7c-df4371b77bbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a sentence to check for depression: h\n",
            "Depression: No\n",
            "Enter a sentence to check for depression: hey wasup\n",
            "Depression: No\n",
            "Enter a sentence to check for depression: i am gonna kill my self this academic pressure\n",
            "Depression: Yes\n",
            "Enter a sentence to check for depression: i am so happy hehe\n",
            "Depression: No\n",
            "Enter a sentence to check for depression: well i am not so happy ;{{{\n",
            "Depression: Yes\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-539894c841f3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a sentence to check for depression: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mpreprocessed_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  user_input = input(\"Enter a sentence to check for depression: \")\n",
        "\n",
        "  preprocessed_input = preprocess_text(user_input)\n",
        "\n",
        "  input_tfidf = vectorizer.transform([preprocessed_input])\n",
        "\n",
        "  prediction = classifier.predict(input_tfidf)[0]\n",
        "\n",
        "  if prediction == 0:\n",
        "    print('Depression: No')\n",
        "  else:\n",
        "    print('Depression: Yes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiOCTN4c5OJM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/TwExtract-elonmusk-20240402_091647.csv', usecols=['tweetText'])\n",
        "\n",
        "total_tweets = len(df)\n",
        "depressed_count = 0\n",
        "\n",
        "for tweet_text in df['tweetText']:\n",
        "    preprocessed_input = preprocess_text(tweet_text)\n",
        "    input_tfidf = vectorizer.transform([preprocessed_input])\n",
        "    prediction = classifier.predict(input_tfidf)[0]\n",
        "    if prediction == 1:  # Assuming 1 indicates depression in your model\n",
        "        depressed_count += 1\n",
        "\n",
        "percentage_depressed = (depressed_count / total_tweets) * 100\n",
        "\n",
        "print(f\"Percentage of depressed tweets: {percentage_depressed:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnOXzs73jFmj",
        "outputId": "52df469f-a573-4865-ca64-5b1da123d69a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "pip install flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "qdmELqaHjHk1",
        "outputId": "61539597-6d08-4574-ab09-1f6534a5dc81"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "URL rule '' must start with a slash.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2a132b60831b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mhello\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"Welcome to machine learning model APIs!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flask/scaffold.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT_route\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT_route\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mendpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"endpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_url_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flask/scaffold.py\u001b[0m in \u001b[0;36mwrapper_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_setup_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/flask/app.py\u001b[0m in \u001b[0;36madd_url_rule\u001b[0;34m(self, rule, endpoint, view_func, provide_automatic_options, **options)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mmethods\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mrequired_methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0mrule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_rule_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0mrule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide_automatic_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprovide_automatic_options\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/werkzeug/routing/rules.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, string, defaults, subdomain, methods, build_only, endpoint, strict_slashes, merge_slashes, redirect_to, alias, host, websocket)\u001b[0m\n\u001b[1;32m    455\u001b[0m     ) -> None:\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"URL rule '{string}' must start with a slash.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: URL rule '' must start with a slash."
          ]
        }
      ],
      "source": [
        "from flask import Flask\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "@app.route(\"\")\n",
        "def hello():\n",
        "    return \"Welcome to machine learning model APIs!\"\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oQc_tIaBY-D",
        "outputId": "7b7ff58f-c0ee-4c6f-a638-f8c1954afe48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation Accuracy: 0.8574375\n",
            "Validation Set Accuracy: 0.8635\n",
            "Test Set Accuracy: 0.861\n",
            "Test Set Precision: 0.8641148325358852\n",
            "Test Set Recall: 0.8691049085659288\n",
            "Test Set F1 Score: 0.8666026871401151\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.sparse import hstack\n",
        "import numpy as np\n",
        "\n",
        "# Function to get sentiment polarity\n",
        "def get_sentiment(text):\n",
        "    from textblob import TextBlob\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Data preparation and splitting into 80-10-10\n",
        "X = df['text']\n",
        "y = df['class']\n",
        "\n",
        "# Initial split: 80% training, 20% validation/test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split temporary set into 10% validation and 10% test\n",
        "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_validation_tfidf = tfidf_vectorizer.transform(X_validation)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Get sentiment polarity and scale\n",
        "X_train_sentiment = np.array([get_sentiment(text) for text in X_train]).reshape(-1, 1)\n",
        "X_validation_sentiment = np.array([get_sentiment(text) for text in X_validation]).reshape(-1, 1)\n",
        "X_test_sentiment = np.array([get_sentiment(text) for text in X_test]).reshape(-1, 1)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_sentiment_scaled = scaler.fit_transform(X_train_sentiment)\n",
        "X_validation_sentiment_scaled = scaler.transform(X_validation_sentiment)\n",
        "X_test_sentiment_scaled = scaler.transform(X_test_sentiment)\n",
        "\n",
        "# Combine TF-IDF with scaled sentiment scores\n",
        "X_train_combined = hstack((X_train_tfidf, X_train_sentiment_scaled))\n",
        "X_validation_combined = hstack((X_validation_tfidf, X_validation_sentiment_scaled))\n",
        "X_test_combined = hstack((X_test_tfidf, X_test_sentiment_scaled))\n",
        "\n",
        "# Hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],  # Inverse of regularization strength\n",
        "    'penalty': ['l1', 'l2'],  # Type of regularization\n",
        "    'solver': ['liblinear'],  # Solver that supports L1/L2\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train_combined, y_train)\n",
        "\n",
        "# Cross-Validation Accuracy\n",
        "cross_val_accuracy = cross_val_score(\n",
        "    LogisticRegression(**grid_search.best_params_),\n",
        "    X_train_combined,\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ").mean()\n",
        "\n",
        "# Train the model with the best hyperparameters\n",
        "logistic_regression_model = LogisticRegression(**grid_search.best_params_)\n",
        "logistic_regression_model.fit(X_train_combined, y_train)\n",
        "\n",
        "# Predict and calculate metrics for the validation set\n",
        "y_val_pred = logistic_regression_model.predict(X_validation_combined)\n",
        "validation_accuracy = accuracy_score(y_validation, y_val_pred)\n",
        "\n",
        "# Predict and calculate metrics for the test set\n",
        "y_test_pred = logistic_regression_model.predict(X_test_combined)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_precision = precision_score(y_test, y_test_pred)\n",
        "test_recall = recall_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "\n",
        "# Output metrics\n",
        "print(\"Cross-Validation Accuracy:\", cross_val_accuracy)\n",
        "print(\"Validation Set Accuracy:\", validation_accuracy)\n",
        "print(\"Test Set Accuracy:\", test_accuracy)\n",
        "print(\"Test Set Precision:\", test_precision)\n",
        "print(\"Test Set Recall:\", test_recall)\n",
        "print(\"Test Set F1 Score:\", test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfbuOE1aPzGW",
        "outputId": "bed7f924-1e37-40a9-ac08-0d058bdafcb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.8503937007874016\n",
            "Recall: 0.8558692421991084\n",
            "F1-score: 0.8531226857566034\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "precision1 = precision_score(y_test, y_pred)\n",
        "\n",
        "recall1 = recall_score(y_test, y_pred)\n",
        "\n",
        "f1_1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision1)\n",
        "print(\"Recall:\", recall1)\n",
        "print(\"F1-score:\", f1_1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}